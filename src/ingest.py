"""
Ğ¡ĞºĞ°Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ FAQ Rust (Ñ€ÑƒÑ/Ğ°Ğ½Ğ³Ğ») Ğ¸Ğ· Ğ¶Ğ¸Ğ²Ğ¾Ğ³Ğ¾ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ° Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ data/rust_faq.csv
Ğ•ÑĞ»Ğ¸ Ğ²ÑĞµ URL Ğ½ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹ â€” Ğ¾ÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ Ñ Ğ¿Ğ¾Ğ½ÑÑ‚Ğ½Ñ‹Ğ¼ RuntimeError.
"""
import csv, requests, sys
from pathlib import Path
from bs4 import BeautifulSoup

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
URLS = [
    # 1) FAQ-Markdown Ğ² ĞºĞ½Ğ¸Ğ³Ğµ Rust â€• main raw URL
    "https://raw.githubusercontent.com/rust-lang/book/main/src/appendix-07-faq.md",   # 200 OK

    # 2) Html-ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ğ° Ñ‚Ğ¾Ğ¹ Ğ¶Ğµ ĞºĞ½Ğ¸Ğ³Ğ¸ (ĞµÑĞ»Ğ¸ raw-Ğ´Ğ¾Ğ¼ĞµĞ½ Ğ²Ğ´Ñ€ÑƒĞ³ Ñ€ĞµĞ¶ĞµÑ‚ÑÑ)
    "https://rust-lang.github.io/book/appendix-07-faq.html",                         # 200 OK

    # 3) Ğ¡Ñ‚Ğ°Ñ€Ğ°Ñ learn/faq (Ğ¸Ğ½Ğ¾Ğ³Ğ´Ğ° 404, Ğ½Ğ¾ Ğ¿ÑƒÑÑ‚ÑŒ Ğ±ÑƒĞ´ĞµÑ‚ Ğ·Ğ°Ğ¿Ğ°Ñ)
    "https://www.rust-lang.org/learn/faq/",                                          # 301â†’200 OK

    # 4) /faq/ ÑĞ¾ ÑĞ»ÑÑˆĞµĞ¼ â€” CDN Ğ¾Ñ‚Ğ´Ğ°Ñ‘Ñ‚ 200 (Ğ±ĞµĞ· ÑĞ»ÑÑˆĞ° Ğ´Ğ°Ñ‘Ñ‚ 422)
    "https://www.rust-lang.org/faq/",                                                # 200 OK
]



HEADERS = {"User-Agent": "Mozilla/5.0 (compatible; mini-rag/0.1)"}
TIMEOUT  = 30

OUT = Path("data/rust_faq.csv")
OUT.parent.mkdir(exist_ok=True)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_first_alive(urls):
    """Ğ’ĞµÑ€Ğ½ÑƒÑ‚ÑŒ (url, BeautifulSoup) Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¹ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²ÑˆĞµĞ¹ÑÑ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹"""
    for url in urls:
        try:
            resp = requests.get(url, headers=HEADERS, timeout=TIMEOUT)
            if resp.ok:
                print(f"âœ…  Ğ‘ĞµÑ€Ñƒ FAQ Ğ¾Ñ‚ÑÑĞ´Ğ°: {url}")
                return url, BeautifulSoup(resp.text, "html.parser")
            print(f"âš ï¸  {url} â†’ HTTP {resp.status_code}")
        except requests.RequestException as e:
            print(f"âš ï¸  {url} â†’ {e.__class__.__name__}")
    raise RuntimeError(
        "ĞĞ¸ Ğ¾Ğ´Ğ¸Ğ½ Ğ¸Ğ· FAQ-URL Ğ½Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ»ÑÑ. "
        "ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑŒÑ‚Ğµ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚ / ĞºĞ¾Ñ€Ğ¿Ğ¾Ñ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾ĞºÑĞ¸."
    )

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def parse_rows(url, soup):
    """Ğ’ĞµÑ€Ğ½ÑƒÑ‚ÑŒ ÑĞ¿Ğ¸ÑĞ¾Ğº ÑĞ»Ğ¾Ğ²Ğ°Ñ€ĞµĞ¹ id/question/answer Ğ¿Ğ¾Ğ´ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½ÑƒÑ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‚ĞºÑƒ"""
    rows = []

    if "/learn/faq" in url:                       # <details><summary> â€¦ </details>
        for i, det in enumerate(soup.select("details"), 1):
            summary = det.find("summary")
            if not summary:
                continue
            q = summary.get_text(" ", strip=True)
            a = det.get_text(" ", strip=True).replace(q, "", 1).strip()
            rows.append({"id": i, "question": q, "answer": a})

    elif url.rstrip("/").endswith("/faq"):        # <dl class="faq"><dt>/<dd>
        for i, dt in enumerate(soup.select("dl.faq dt"), 1):
            dd = dt.find_next_sibling("dd")
            if not dd:
                continue
            rows.append({
                "id": i,
                "question": dt.get_text(" ", strip=True),
                "answer":   dd.get_text(" ", strip=True)
            })

    elif url.endswith("faq.md") or url.endswith("faq.html"):
        # Ğ”Ğ»Ñ .md Ğ±ĞµÑ€Ñ‘Ğ¼ Ñ‡Ğ¸ÑÑ‚Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚, Ğ´Ğ»Ñ .html ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ²Ñ‹Ñ‚Ğ°Ñ‰Ğ¸Ğ¼ Ñ‚ĞµĞºÑÑ‚ Ğ¸Ğ· <body>
        text_block = soup.get_text(" ", strip=True)
        import re
        sections = re.split(r"###\\s+", text_block)   # Ğ² Markdown/HTML FAQ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ Q Ğ½Ğ°Ñ‡Ğ¸Ğ½Ğ°ĞµÑ‚ÑÑ Ñ ### Ğ’Ğ¾Ğ¿Ñ€Ğ¾Ñ
        for i, chunk in enumerate(sections[1:], 1):   # [0] â€” Ğ²Ğ²Ğ¾Ğ´Ğ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚ Ğ´Ğ¾ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ ###
            q, *a = chunk.splitlines()
            rows.append({
                "id": i,
                "question": q.strip(),
                "answer":   "\n".join(a).strip(),
            })

    if not rows:
        raise RuntimeError(
            "ĞŸĞ°Ñ€ÑĞµÑ€ Ğ½Ğµ Ğ½Ğ°ÑˆÑ‘Ğ» Ğ½Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°: "
            "Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ ÑĞ½Ğ¾Ğ²Ğ° Ğ¸Ğ·Ğ¼ĞµĞ½Ğ¸Ğ»Ğ°ÑÑŒ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° HTML."
        )
    return rows

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    url, soup = fetch_first_alive(URLS)
    rows      = parse_rows(url, soup)
    print("ĞĞ°Ğ¹Ğ´ĞµĞ½Ğ¾ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²:", len(rows))

    with OUT.open("w", newline="", encoding="utf-8") as f:
        w = csv.DictWriter(f, fieldnames=rows[0].keys())
        w.writeheader()
        w.writerows(rows)
    print(f"ğŸ’¾  Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¾ Ğ² {OUT}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        sys.exit(f"ğŸ›‘  {e}")
